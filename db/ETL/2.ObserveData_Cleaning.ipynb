{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning\n",
    "**Data Cleaning Function (`clean_isd_lite_data`)**:\n",
    "   - Iterates through JSON records and extracts relevant data such as year, month, day, hour, temperature, dew point temperature, pressure, wind direction, wind speed, cloud cover, rain 1-hour, and rain 6-hour.\n",
    "   - Scales temperature, dew point temperature, pressure, rain 1-hour, and rain 6-hour values and handles missing values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20b922f50a8980e3"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 997990-99999-2020-2024.json\n",
      "Processed 721012-99999-2020-2024.json\n",
      "Processed 722704-99999-2020-2024.json\n",
      "Processed 997787-99999-2020-2024.json\n",
      "Processed 720907-99999-2020-2024.json\n",
      "Processed 720642-99999-2020-2024.json\n",
      "Processed 994082-99999-2020-2023.json\n",
      "Processed 997272-99999-2020-2021.json\n",
      "Processed 720449-99999-2020-2024.json\n",
      "Processed 994061-99999-2023-2024.json\n",
      "Processed 722158-99999-2020-2023.json\n",
      "Processed 998223-99999-2020-2024.json\n",
      "Processed 998274-99999-2020-2023.json\n",
      "Processed 997988-99999-2020-2024.json\n",
      "Processed 997262-99999-2020-2024.json\n",
      "Processed 720451-99999-2020-2024.json\n",
      "Processed 724015-99999-2020-2024.json\n",
      "Processed 997255-99999-2020-2024.json\n",
      "Processed 722592-99999-2020-2024.json\n",
      "Processed 720129-99999-2020-2024.json\n",
      "Processed 720995-99999-2020-2024.json\n",
      "Processed 747960-99999-2020-2024.json\n",
      "Processed 994971-99999-2020-2024.json\n",
      "Processed 720409-99999-2020-2024.json\n",
      "Processed 997802-99999-2020-2024.json\n",
      "Processed 721013-99999-2020-2024.json\n",
      "Processed 720511-99999-2021-2024.json\n",
      "Processed 997994-99999-2020-2024.json\n",
      "Processed 997254-99999-2020-2024.json\n",
      "Processed 994006-99999-2020-2022.json\n",
      "Processed 720994-99999-2020-2024.json\n",
      "Processed 997687-99999-2020-2024.json\n",
      "Processed 997266-99999-2020-2024.json\n",
      "Processed 998409-99999-2020-2024.json\n",
      "Processed 720329-99999-2020-2024.json\n",
      "Processed 998436-99999-2024-2024.json\n",
      "Processed 720507-99999-2020-2024.json\n",
      "Processed 997290-99999-2020-2024.json\n",
      "Processed 997338-99999-2020-2024.json\n",
      "Processed 998224-99999-2020-2024.json\n",
      "Processed 997260-99999-2020-2024.json\n",
      "Processed 997300-99999-2020-2024.json\n",
      "Processed 997258-99999-2020-2024.json\n",
      "Processed 997296-99999-2020-2024.json\n",
      "Processed 997265-99999-2020-2024.json\n",
      "Processed 998219-99999-2020-2024.json\n",
      "Processed 998017-99999-2020-2024.json\n",
      "Processed 720978-99999-2020-2023.json\n",
      "Processed 998216-99999-2020-2024.json\n",
      "Processed 721043-99999-2020-2024.json\n",
      "Processed 724720-99999-2020-2024.json\n",
      "Processed 997257-99999-2020-2024.json\n",
      "Processed 720572-99999-2020-2024.json\n",
      "Processed 720681-99999-2020-2024.json\n",
      "Processed 997359-99999-2020-2024.json\n",
      "Processed 720979-99999-2020-2024.json\n",
      "Processed 720516-99999-2020-2024.json\n",
      "Processed 997381-99999-2020-2024.json\n",
      "Processed 720996-99999-2020-2024.json\n",
      "Processed 998278-99999-2020-2023.json\n",
      "Processed 997259-99999-2020-2024.json\n",
      "Processed 998277-99999-2020-2023.json\n",
      "Processed 997261-99999-2020-2024.json\n",
      "Processed 701338-99999-2020-2024.json\n",
      "Processed 997264-99999-2020-2024.json\n",
      "Processed 997297-99999-2020-2024.json\n",
      "Processed 720724-99999-2020-2024.json\n",
      "Processed 720445-99999-2020-2024.json\n",
      "Processed 721011-99999-2020-2024.json\n",
      "Processed 720977-99999-2020-2024.json\n",
      "Processed 997360-99999-2020-2024.json\n",
      "Runtime: 25.894346952438354 seconds\n"
     ]
    }
   ],
   "source": [
    "'''STEP1: Data Cleaning'''\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Define reasonable ranges for each meteorological indicator based on the month\n",
    "ranges = {\n",
    "    'Temperature': {\n",
    "        1: (-10, 5), 2: (-5, 10), 3: (0, 15), 4: (5, 20), 5: (10, 25), 6: (15, 30), \n",
    "        7: (20, 35), 8: (20, 35), 9: (15, 30), 10: (10, 25), 11: (0, 15), 12: (-5, 10)\n",
    "    },\n",
    "    'DewPointTemperature': {\n",
    "        1: (-15, 0), 2: (-10, 5), 3: (-5, 10), 4: (0, 15), 5: (5, 20), 6: (10, 25), \n",
    "        7: (15, 30), 8: (15, 30), 9: (10, 25), 10: (5, 20), 11: (-5, 10), 12: (-10, 5)\n",
    "    },\n",
    "    'Pressure': (950, 1050),  # Unified pressure range\n",
    "    'WindSpeed': (0, 20),     # Unified wind speed range\n",
    "    'CloudCover': (0, 8),     # Unified cloud cover range\n",
    "    'Rain1h': (0, 10),        # Unified 1-hour precipitation range\n",
    "    'Rain6h': (0, 60)         # Unified 6-hour precipitation range\n",
    "}\n",
    "\n",
    "# Function to determine event_id based on weather data\n",
    "def determine_event_id(record):\n",
    "    temperature = record['Temperature']\n",
    "    dew_point = record['DewPointTemperature']\n",
    "    pressure = record['Pressure']\n",
    "    wind_speed = record['WindSpeed']\n",
    "    cloud_cover = record['CloudCover']\n",
    "    rain1h = record['Rain1h']\n",
    "    rain6h = record['Rain6h']\n",
    "    \n",
    "    if temperature >= 30:\n",
    "        return 1  # Heat Wave\n",
    "    elif temperature <= -10:\n",
    "        return 2  # Cold Wave\n",
    "    elif wind_speed >= 20:\n",
    "        return 3  # High Wind\n",
    "    elif rain6h >= 10:\n",
    "        return 5  # Heavy Rainfall\n",
    "    elif rain1h >= 50:\n",
    "        return 5  # Heavy Rainfall\n",
    "    # Placeholder logic for other events; update based on actual conditions\n",
    "    elif cloud_cover > 6:\n",
    "        return 7  # Thunderstorm (assumption)\n",
    "    elif pressure < 960:\n",
    "        return 9  # Pressure Drop (assumption)\n",
    "    else:\n",
    "        return 0  # No specific event\n",
    "\n",
    "# Data cleaning function\n",
    "def clean_isd_lite_data(json_data):\n",
    "    cleaned_data = []\n",
    "\n",
    "    for record in json_data:\n",
    "        cleaned_record = {}\n",
    "        month = record['Month']\n",
    "        cleaned_record['Year'] = record['Year']\n",
    "        cleaned_record['Month'] = month\n",
    "        cleaned_record['Day'] = record['Day']\n",
    "        cleaned_record['Hour'] = record['Hour']\n",
    "\n",
    "        # Temperature, Dew Point Temperature, and Sea Level Pressure: Scale and handle missing values\n",
    "        cleaned_record['Temperature'] = record['Temperature'] / 10.0 if record['Temperature'] != -9999 else round(random.uniform(*ranges['Temperature'][month]), 1)\n",
    "        cleaned_record['DewPointTemperature'] = record['Td'] / 10.0 if record['Td'] != -9999 else round(random.uniform(*ranges['DewPointTemperature'][month]), 1)\n",
    "        cleaned_record['Pressure'] = record['Pressure'] / 10.0 if record['Pressure'] != -9999 else round(random.uniform(*ranges['Pressure']), 1)\n",
    "\n",
    "        # Wind Direction and Speed: Handle missing values\n",
    "        cleaned_record['WindDirection'] = record['WindDirection'] if record['WindDirection'] != -9999 else round(random.uniform(0, 360), 1)\n",
    "        cleaned_record['WindSpeed'] = record['WindSpeed'] / 10.0 if record['WindSpeed'] != -9999 else round(random.uniform(*ranges['WindSpeed']), 1)\n",
    "\n",
    "        # Cloud Cover: Handle missing values\n",
    "        cleaned_record['CloudCover'] = record['CloudCover'] if record['CloudCover'] != -9999 else round(random.uniform(*ranges['CloudCover']), 1)\n",
    "\n",
    "        # Precipitation: Scale and handle missing and trace values\n",
    "        cleaned_record['Rain1h'] = record['Rain1h'] / 10.0 if record['Rain1h'] not in [-9999, -1] else (0 if record['Rain1h'] == -1 else round(random.uniform(*ranges['Rain1h']), 1))\n",
    "        cleaned_record['Rain6h'] = record['Rain6h'] / 10.0 if record['Rain6h'] not in [-9999, -1] else (0 if record['Rain6h'] == -1 else round(random.uniform(*ranges['Rain6h']), 1))\n",
    "\n",
    "        # Determine event_id\n",
    "        cleaned_record['event_id'] = determine_event_id(cleaned_record)\n",
    "\n",
    "        cleaned_data.append(cleaned_record)\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Function to read and clean JSON files\n",
    "def read_and_clean_json_files(base_directory):\n",
    "    # Iterate through all JSON files in the base directory\n",
    "    for filename in os.listdir(base_directory):\n",
    "        if filename.endswith('.json') and not filename.startswith('cleaned_'):\n",
    "            file_path = os.path.join(base_directory, filename)\n",
    "            \n",
    "            # Read JSON data\n",
    "            with open(file_path, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "            \n",
    "            # Clean the data\n",
    "            cleaned_data = clean_isd_lite_data(json_data)\n",
    "            \n",
    "            print(f\"Processed {filename}\")\n",
    "            # Save the cleaned data as a new JSON file\n",
    "            cleaned_file_path = os.path.join(base_directory, f\"cleaned_{filename}\")\n",
    "            with open(cleaned_file_path, 'w') as cleaned_file:\n",
    "                json.dump(cleaned_data, cleaned_file, indent=4)\n",
    "            \n",
    "            # Remove the original JSON file after cleaning\n",
    "            os.remove(file_path)\n",
    "\n",
    "# Main program\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    # Specify the base directory path\n",
    "    base_directory = '/Users/a1234/Desktop/workspace/CS779/WeatherDB/dataset/2020_2024_US'\n",
    "    read_and_clean_json_files(base_directory)\n",
    "    # Record the end time\n",
    "    end_time = time.time()\n",
    "    # Calculate and print the runtime\n",
    "    runtime1 = end_time - start_time\n",
    "    print(f\"Runtime: {runtime1} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-05T17:38:27.396039Z",
     "start_time": "2024-08-05T17:38:01.418840Z"
    }
   },
   "id": "8c88aad818c838eb"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed cleaned_720507-99999-2020-2024.json\n",
      "Processed cleaned_997290-99999-2020-2024.json\n",
      "Processed cleaned_998409-99999-2020-2024.json\n",
      "Processed cleaned_720329-99999-2020-2024.json\n",
      "Processed cleaned_998436-99999-2024-2024.json\n",
      "Processed cleaned_997266-99999-2020-2024.json\n",
      "Processed cleaned_997687-99999-2020-2024.json\n",
      "Processed cleaned_720994-99999-2020-2024.json\n",
      "Processed cleaned_997254-99999-2020-2024.json\n",
      "Processed cleaned_994006-99999-2020-2022.json\n",
      "Processed cleaned_997994-99999-2020-2024.json\n",
      "Processed cleaned_720511-99999-2021-2024.json\n",
      "Processed cleaned_721013-99999-2020-2024.json\n",
      "Processed cleaned_997802-99999-2020-2024.json\n",
      "Processed cleaned_720409-99999-2020-2024.json\n",
      "Processed cleaned_994971-99999-2020-2024.json\n",
      "Processed cleaned_747960-99999-2020-2024.json\n",
      "Processed cleaned_722592-99999-2020-2024.json\n",
      "Processed cleaned_720129-99999-2020-2024.json\n",
      "Processed cleaned_720995-99999-2020-2024.json\n",
      "Processed cleaned_997255-99999-2020-2024.json\n",
      "Processed cleaned_720451-99999-2020-2024.json\n",
      "Processed cleaned_724015-99999-2020-2024.json\n",
      "Processed cleaned_997988-99999-2020-2024.json\n",
      "Processed cleaned_997262-99999-2020-2024.json\n",
      "Processed cleaned_998274-99999-2020-2023.json\n",
      "Processed cleaned_722158-99999-2020-2023.json\n",
      "Processed cleaned_998223-99999-2020-2024.json\n",
      "Processed cleaned_994061-99999-2023-2024.json\n",
      "Processed cleaned_720449-99999-2020-2024.json\n",
      "Processed cleaned_997272-99999-2020-2021.json\n",
      "Processed cleaned_720642-99999-2020-2024.json\n",
      "Processed cleaned_994082-99999-2020-2023.json\n",
      "Processed cleaned_997787-99999-2020-2024.json\n",
      "Processed cleaned_720907-99999-2020-2024.json\n",
      "Processed cleaned_722704-99999-2020-2024.json\n",
      "Processed cleaned_997990-99999-2020-2024.json\n",
      "Processed cleaned_721012-99999-2020-2024.json\n",
      "Processed cleaned_720977-99999-2020-2024.json\n",
      "Processed cleaned_997360-99999-2020-2024.json\n",
      "Processed cleaned_720445-99999-2020-2024.json\n",
      "Processed cleaned_721011-99999-2020-2024.json\n",
      "Processed cleaned_720724-99999-2020-2024.json\n",
      "Processed cleaned_997264-99999-2020-2024.json\n",
      "Processed cleaned_997297-99999-2020-2024.json\n",
      "Processed cleaned_997261-99999-2020-2024.json\n",
      "Processed cleaned_701338-99999-2020-2024.json\n",
      "Processed cleaned_997259-99999-2020-2024.json\n",
      "Processed cleaned_998277-99999-2020-2023.json\n",
      "Processed cleaned_998278-99999-2020-2023.json\n",
      "Processed cleaned_997381-99999-2020-2024.json\n",
      "Processed cleaned_720996-99999-2020-2024.json\n",
      "Processed cleaned_720516-99999-2020-2024.json\n",
      "Processed cleaned_720979-99999-2020-2024.json\n",
      "Processed cleaned_997359-99999-2020-2024.json\n",
      "Processed cleaned_720681-99999-2020-2024.json\n",
      "Processed cleaned_720572-99999-2020-2024.json\n",
      "Processed cleaned_997257-99999-2020-2024.json\n",
      "Processed cleaned_724720-99999-2020-2024.json\n",
      "Processed cleaned_998216-99999-2020-2024.json\n",
      "Processed cleaned_721043-99999-2020-2024.json\n",
      "Processed cleaned_998219-99999-2020-2024.json\n",
      "Processed cleaned_998017-99999-2020-2024.json\n",
      "Processed cleaned_720978-99999-2020-2023.json\n",
      "Processed cleaned_997296-99999-2020-2024.json\n",
      "Processed cleaned_997265-99999-2020-2024.json\n",
      "Processed cleaned_997300-99999-2020-2024.json\n",
      "Processed cleaned_997258-99999-2020-2024.json\n",
      "Processed cleaned_997338-99999-2020-2024.json\n",
      "Processed cleaned_998224-99999-2020-2024.json\n",
      "Processed cleaned_997260-99999-2020-2024.json\n",
      "Runtime: 27.189810037612915 seconds\n"
     ]
    }
   ],
   "source": [
    "'''STEP2: create station_id，observer_id，observation_device_id'''\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Function to extract station ID from filename\n",
    "def extract_station_id(filename):\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    parts = base_name.split('_')\n",
    "    if len(parts) == 2 and parts[0] == 'cleaned':\n",
    "        # Split by hyphen and take the first two parts to form the station ID\n",
    "        station_parts = parts[1].split('-')\n",
    "        if len(station_parts) >= 2:\n",
    "            station_id = station_parts[0] + station_parts[1]\n",
    "            # Convert station_id to integer\n",
    "            try:\n",
    "                station_id = int(station_id)\n",
    "                return station_id\n",
    "            except ValueError:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "# Function to add station ID and random fields to data\n",
    "def add_fields_to_data(json_data, station_id):\n",
    "    updated_data = []\n",
    "\n",
    "    for record in json_data:\n",
    "        updated_record = {}\n",
    "\n",
    "        # Add station ID\n",
    "        updated_record['station_id'] = station_id\n",
    "\n",
    "        # Retain original fields\n",
    "        updated_record.update(record)\n",
    "\n",
    "        # Add observer_id and observation_device_id\n",
    "        updated_record['observer_id'] = random.choice([1, 7, 13, 19, 25, 31])\n",
    "        updated_record['observation_device_id'] = random.randint(1, 10)\n",
    "\n",
    "        updated_data.append(updated_record)\n",
    "\n",
    "    return updated_data\n",
    "\n",
    "def update_json_files(base_directory):\n",
    "    # Iterate through all JSON files in the base directory\n",
    "    for filename in os.listdir(base_directory):\n",
    "        if filename.endswith('.json') and filename.startswith('cleaned_'):\n",
    "            file_path = os.path.join(base_directory, filename)\n",
    "            station_id = extract_station_id(filename)\n",
    "            if station_id is None:\n",
    "                continue\n",
    "            \n",
    "            # Read JSON data\n",
    "            with open(file_path, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "            \n",
    "            # Add new fields\n",
    "            updated_data = add_fields_to_data(json_data, station_id)\n",
    "            \n",
    "            print(f\"Processed {filename}\")\n",
    "            # Save the updated data to a new JSON file\n",
    "            updated_file_path = os.path.join(base_directory, f\"updated_{filename}\")\n",
    "            with open(updated_file_path, 'w') as updated_file:\n",
    "                json.dump(updated_data, updated_file, indent=4)\n",
    "\n",
    "            # Remove the original cleaned JSON file\n",
    "            os.remove(file_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    # Specify the base_directory path\n",
    "    base_directory = '/Users/a1234/Desktop/workspace/CS779/WeatherDB/dataset/2020_2024_US'\n",
    "    update_json_files(base_directory)\n",
    "    # Record the end time\n",
    "    end_time = time.time()\n",
    "    # Calculate and print the runtime\n",
    "    runtime = end_time - start_time\n",
    "    print(f\"Runtime: {runtime} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-05T17:38:57.614410Z",
     "start_time": "2024-08-05T17:38:30.421532Z"
    }
   },
   "id": "e60d28b485c96a97"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 5.526558876037598 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "def extract_time_data(json_data):\n",
    "    time_data = []\n",
    "\n",
    "    for record in json_data:\n",
    "        year = record.get('Year')\n",
    "        month = record.get('Month')\n",
    "        day = record.get('Day')\n",
    "        hour = record.get('Hour')\n",
    "        \n",
    "        if year is not None and month is not None and day is not None and hour is not None:\n",
    "            time_data.append({\n",
    "                'Year': year,\n",
    "                'Month': month,\n",
    "                'Day': day,\n",
    "                'Hour': hour\n",
    "            })\n",
    "\n",
    "    return time_data\n",
    "\n",
    "def determine_quarter(month):\n",
    "    if 1 <= month <= 3:\n",
    "        return 'Q1'\n",
    "    elif 4 <= month <= 6:\n",
    "        return 'Q2'\n",
    "    elif 7 <= month <= 9:\n",
    "        return 'Q3'\n",
    "    elif 10 <= month <= 12:\n",
    "        return 'Q4'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def determine_day_session(hour):\n",
    "    if 0 <= hour < 6:\n",
    "        return 'Night'\n",
    "    elif 6 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'Afternoon'\n",
    "    elif 18 <= hour < 24:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_time_json(base_directory, output_file):\n",
    "    all_time_data = set()\n",
    "    unique_time_data = []\n",
    "\n",
    "    # Iterate through all updated JSON files in the base directory\n",
    "    for filename in os.listdir(base_directory):\n",
    "        if filename.endswith('.json') and filename.startswith('updated_'):\n",
    "            file_path = os.path.join(base_directory, filename)\n",
    "\n",
    "            # Read JSON data\n",
    "            with open(file_path, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "            \n",
    "            # Extract time data\n",
    "            time_data = extract_time_data(json_data)\n",
    "\n",
    "            for entry in time_data:\n",
    "                year = entry['Year']\n",
    "                month = entry['Month']\n",
    "                day = entry['Day']\n",
    "                hour = entry['Hour']\n",
    "\n",
    "                # Create a tuple to check for duplicates\n",
    "                time_tuple = (year, month, day, hour)\n",
    "                \n",
    "                if time_tuple not in all_time_data:\n",
    "                    all_time_data.add(time_tuple)\n",
    "                    unique_time_data.append({\n",
    "                        'Year': year,\n",
    "                        'Month': month,\n",
    "                        'Day': day,\n",
    "                        'Hour': hour\n",
    "                    })\n",
    "\n",
    "    # Sort the unique time data\n",
    "    unique_time_data = sorted(unique_time_data, key=lambda x: (x['Year'], x['Month'], x['Day'], x['Hour']))\n",
    "\n",
    "    # Add time_id and other derived fields\n",
    "    for time_id_counter, entry in enumerate(unique_time_data, start=1):\n",
    "        entry.update({\n",
    "            'time_id': time_id_counter,\n",
    "            'quarter': determine_quarter(entry['Month']),\n",
    "            'day_session': determine_day_session(entry['Hour'])\n",
    "        })\n",
    "\n",
    "    # Save the sorted and updated unique time data to a new JSON file\n",
    "    with open(output_file, 'w') as out_file:\n",
    "        json.dump(unique_time_data, out_file, indent=4)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    # Specify the base_directory path and output file path\n",
    "    base_directory = '/Users/a1234/Desktop/workspace/CS779/WeatherDB/dataset/2020_2024_US'\n",
    "    output_file = '/Users/a1234/Desktop/workspace/CS779/WeatherDB/dataset/time.json'\n",
    "    generate_time_json(base_directory, output_file)\n",
    "    # Record the end time\n",
    "    end_time = time.time()\n",
    "    # Calculate and print the runtime\n",
    "    runtime = end_time - start_time\n",
    "    print(f\"Runtime: {runtime} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:09:02.595970Z",
     "start_time": "2024-08-06T07:08:57.059395Z"
    }
   },
   "id": "aa6d309f6a743726"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time mapping...\n",
      "Updating observations with time_id...\n",
      "Processed updated_cleaned_998274-99999-2020-2023.json\n",
      "Processed updated_cleaned_997988-99999-2020-2024.json\n",
      "Processed updated_cleaned_997262-99999-2020-2024.json\n",
      "Processed updated_cleaned_994061-99999-2023-2024.json\n",
      "Processed updated_cleaned_722158-99999-2020-2023.json\n",
      "Processed updated_cleaned_998223-99999-2020-2024.json\n",
      "Processed updated_cleaned_720995-99999-2020-2024.json\n",
      "Processed updated_cleaned_722592-99999-2020-2024.json\n",
      "Processed updated_cleaned_720129-99999-2020-2024.json\n",
      "Processed updated_cleaned_724015-99999-2020-2024.json\n",
      "Processed updated_cleaned_720451-99999-2020-2024.json\n",
      "Processed updated_cleaned_997255-99999-2020-2024.json\n",
      "Processed updated_cleaned_721012-99999-2020-2024.json\n",
      "Processed updated_cleaned_997990-99999-2020-2024.json\n",
      "Processed updated_cleaned_722704-99999-2020-2024.json\n",
      "Processed updated_cleaned_997272-99999-2020-2021.json\n",
      "Processed updated_cleaned_720449-99999-2020-2024.json\n",
      "Processed updated_cleaned_720907-99999-2020-2024.json\n",
      "Processed updated_cleaned_997787-99999-2020-2024.json\n",
      "Processed updated_cleaned_720642-99999-2020-2024.json\n",
      "Processed updated_cleaned_994082-99999-2020-2023.json\n",
      "Processed updated_cleaned_720994-99999-2020-2024.json\n",
      "Processed updated_cleaned_997687-99999-2020-2024.json\n",
      "Processed updated_cleaned_997254-99999-2020-2024.json\n",
      "Processed updated_cleaned_994006-99999-2020-2022.json\n",
      "Processed updated_cleaned_997290-99999-2020-2024.json\n",
      "Processed updated_cleaned_720507-99999-2020-2024.json\n",
      "Processed updated_cleaned_997266-99999-2020-2024.json\n",
      "Processed updated_cleaned_720329-99999-2020-2024.json\n",
      "Processed updated_cleaned_998436-99999-2024-2024.json\n",
      "Processed updated_cleaned_998409-99999-2020-2024.json\n",
      "Processed updated_cleaned_994971-99999-2020-2024.json\n",
      "Processed updated_cleaned_720409-99999-2020-2024.json\n",
      "Processed updated_cleaned_747960-99999-2020-2024.json\n",
      "Processed updated_cleaned_720511-99999-2021-2024.json\n",
      "Processed updated_cleaned_997994-99999-2020-2024.json\n",
      "Processed updated_cleaned_997802-99999-2020-2024.json\n",
      "Processed updated_cleaned_721013-99999-2020-2024.json\n",
      "Processed updated_cleaned_720681-99999-2020-2024.json\n",
      "Processed updated_cleaned_997359-99999-2020-2024.json\n",
      "Processed updated_cleaned_720516-99999-2020-2024.json\n",
      "Processed updated_cleaned_720979-99999-2020-2024.json\n",
      "Processed updated_cleaned_997265-99999-2020-2024.json\n",
      "Processed updated_cleaned_997296-99999-2020-2024.json\n",
      "Processed updated_cleaned_720978-99999-2020-2023.json\n",
      "Processed updated_cleaned_998017-99999-2020-2024.json\n",
      "Processed updated_cleaned_998219-99999-2020-2024.json\n",
      "Processed updated_cleaned_997260-99999-2020-2024.json\n",
      "Processed updated_cleaned_998224-99999-2020-2024.json\n",
      "Processed updated_cleaned_997338-99999-2020-2024.json\n",
      "Processed updated_cleaned_997258-99999-2020-2024.json\n",
      "Processed updated_cleaned_997300-99999-2020-2024.json\n",
      "Processed updated_cleaned_997257-99999-2020-2024.json\n",
      "Processed updated_cleaned_720572-99999-2020-2024.json\n",
      "Processed updated_cleaned_721043-99999-2020-2024.json\n",
      "Processed updated_cleaned_998216-99999-2020-2024.json\n",
      "Processed updated_cleaned_724720-99999-2020-2024.json\n",
      "Processed updated_cleaned_720724-99999-2020-2024.json\n",
      "Processed updated_cleaned_721011-99999-2020-2024.json\n",
      "Processed updated_cleaned_720445-99999-2020-2024.json\n",
      "Processed updated_cleaned_997360-99999-2020-2024.json\n",
      "Processed updated_cleaned_720977-99999-2020-2024.json\n",
      "Processed updated_cleaned_998278-99999-2020-2023.json\n",
      "Processed updated_cleaned_720996-99999-2020-2024.json\n",
      "Processed updated_cleaned_997381-99999-2020-2024.json\n",
      "Processed updated_cleaned_997297-99999-2020-2024.json\n",
      "Processed updated_cleaned_997264-99999-2020-2024.json\n",
      "Processed updated_cleaned_998277-99999-2020-2023.json\n",
      "Processed updated_cleaned_997259-99999-2020-2024.json\n",
      "Processed updated_cleaned_701338-99999-2020-2024.json\n",
      "Processed updated_cleaned_997261-99999-2020-2024.json\n",
      "Runtime: 23.957683086395264 seconds\n",
      "Updated observations saved to: /Users/a1234/Desktop/workspace/CS779/WeatherDB/dataset/observation.json\n"
     ]
    }
   ],
   "source": [
    "'''STEP4: Merge all observations into a single json file'''\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "def load_time_mapping(time_file):\n",
    "    with open(time_file, 'r') as file:\n",
    "        time_data = json.load(file)\n",
    "    \n",
    "    time_mapping = {}\n",
    "    for entry in time_data:\n",
    "        time_tuple = (entry['Year'], entry['Month'], entry['Day'], entry['Hour'])\n",
    "        time_mapping[time_tuple] = entry['time_id']\n",
    "    \n",
    "    return time_mapping\n",
    "\n",
    "def update_observations(base_directory, time_mapping, output_file):\n",
    "    all_observations = []\n",
    "\n",
    "    # Iterate through all updated JSON files in the base directory\n",
    "    for filename in os.listdir(base_directory):\n",
    "        if filename.endswith('.json') and filename.startswith('updated_'):\n",
    "            file_path = os.path.join(base_directory, filename)\n",
    "\n",
    "            # Read JSON data\n",
    "            with open(file_path, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "            \n",
    "            # Update each record with the corresponding time_id\n",
    "            for record in json_data:\n",
    "                year = record.pop('Year', None)\n",
    "                month = record.pop('Month', None)\n",
    "                day = record.pop('Day', None)\n",
    "                hour = record.pop('Hour', None)\n",
    "\n",
    "                if year is not None and month is not None and day is not None and hour is not None:\n",
    "                    time_tuple = (year, month, day, hour)\n",
    "                    time_id = time_mapping.get(time_tuple)\n",
    "\n",
    "                    if time_id is not None:\n",
    "                        # Ensure time_id is the first field in the record\n",
    "                        updated_record = {'time_id': time_id}\n",
    "                        updated_record.update(record)\n",
    "                        all_observations.append(updated_record)\n",
    "            \n",
    "            print(f\"Processed {filename}\")\n",
    "\n",
    "    # Save the updated observations to a new JSON file\n",
    "    with open(output_file, 'w') as out_file:\n",
    "        json.dump(all_observations, out_file, indent=4)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    # Specify the base_directory path and the paths for time and observation files\n",
    "    base_directory = '/Users/a1234/Desktop/workspace/CS779/WeatherDB/dataset/2020_2024_US'\n",
    "    time_file = '/Users/a1234/Desktop/workspace/CS779/WeatherDB/dataset/time.json'\n",
    "    output_file = '/Users/a1234/Desktop/workspace/CS779/WeatherDB/dataset/observation.json'\n",
    "    \n",
    "    print(\"Loading time mapping...\")\n",
    "    # Load the time mapping from time.json\n",
    "    time_mapping = load_time_mapping(time_file)\n",
    "    \n",
    "    print(\"Updating observations with time_id...\")\n",
    "    # Update the observations with the time_id\n",
    "    update_observations(base_directory, time_mapping, output_file)\n",
    "    \n",
    "    # Record the end time\n",
    "    end_time = time.time()\n",
    "    # Calculate and print the runtime\n",
    "    runtime = end_time - start_time\n",
    "    print(f\"Runtime: {runtime} seconds\")\n",
    "    print(f\"Updated observations saved to: {output_file}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:11:07.451939Z",
     "start_time": "2024-08-06T07:10:43.490542Z"
    }
   },
   "id": "2bb31114e69c3c03"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
